# -*- coding: utf-8 -*-
"""
RAG API cho Tendoo Documentation
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional, Any
import numpy as np
import google.generativeai as genai
import ollama
from sklearn.metrics.pairwise import cosine_similarity
import json
import os
from pathlib import Path


app = FastAPI(title="Tendoo RAG API")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Configuration
class Config:
    GEMINI_API_KEY = "AIzaSyBM2d63oq4whZNtQMxrCUd1KDtVItsSALA"
    LLM_MODEL = "gemini-2.5-flash"
    EMBEDDING_MODEL = "bge-m3"
    CHUNKS_FILE = "output_tendoo/chunks.json"

    # RAG parameters
    TOP_K = 5
    MAX_DESCENDANTS = 5
    MAX_SIBLINGS = 3
    INCLUDE_PARENT = True


config = Config()

# Configure Gemini API
genai.configure(api_key=config.GEMINI_API_KEY)


# In-memory storage
vector_store = {
    "chunks": [],
    "embeddings": [],
    "chunk_map": {}
}


# Pydantic Models
class QueryRequest(BaseModel):
    query: str
    topK: int = 5


class QueryResponse(BaseModel):
    answer: str
    retrievedDocuments: List[Dict[str, Any]]
    metadata: Optional[Dict[str, Any]] = None


# ==================== HELPER FUNCTIONS ====================

def load_chunks():
    """Load chunks t·ª´ file JSON"""
    chunks_file = config.CHUNKS_FILE

    if not os.path.exists(chunks_file):
        print(f"‚ùå File {chunks_file} kh√¥ng t·ªìn t·∫°i!")
        print("H√£y ch·∫°y test_tendoo.py tr∆∞·ªõc ƒë·ªÉ t·∫°o chunks.")
        return []

    with open(chunks_file, 'r', encoding='utf-8') as f:
        chunks = json.load(f)

    print(f"‚úÖ ƒê√£ load {len(chunks)} chunks t·ª´ {chunks_file}")
    return chunks


def create_embedding(text: str) -> List[float]:
    """T·∫°o embedding s·ª≠ d·ª•ng Ollama"""
    try:
        response = ollama.embeddings(
            model=config.EMBEDDING_MODEL,
            prompt=text
        )
        return response['embedding']
    except Exception as e:
        print(f"‚ùå L·ªói t·∫°o embedding: {e}")
        return []


def embed_chunks(chunks: List[Dict]):
    """T·∫°o embeddings cho t·∫•t c·∫£ chunks"""
    print("\nüîÑ ƒêang t·∫°o embeddings cho chunks...")

    embeddings = []
    for i, chunk in enumerate(chunks):
        # T·∫°o text ƒë·ªÉ embed (ti√™u ƒë·ªÅ + n·ªôi dung)
        text_to_embed = f"{chunk['metadata']['section_title']}\n{chunk['content']}"
        embedding = create_embedding(text_to_embed)

        if embedding:
            embeddings.append(embedding)
        else:
            # N·∫øu l·ªói, t·∫°o zero vector
            embeddings.append([0.0] * 1024)

        if (i + 1) % 10 == 0:
            print(f"  ƒê√£ embed {i + 1}/{len(chunks)} chunks")

    print(f"‚úÖ ƒê√£ t·∫°o {len(embeddings)} embeddings")
    return embeddings


def retrieve_similar_chunks(query_embedding: List[float], top_k: int = 5) -> List[Dict]:
    """T√¨m c√°c chunks t∆∞∆°ng ƒë·ªìng nh·∫•t"""
    if not vector_store["embeddings"]:
        return []

    # T√≠nh cosine similarity
    query_emb = np.array(query_embedding).reshape(1, -1)
    chunk_embs = np.array(vector_store["embeddings"])

    similarities = cosine_similarity(query_emb, chunk_embs)[0]

    # L·∫•y top K
    top_indices = np.argsort(similarities)[::-1][:top_k]

    results = []
    for idx in top_indices:
        chunk = vector_store["chunks"][idx]
        results.append({
            "chunk": chunk,
            "similarity": float(similarities[idx])
        })

    return results


def enrich_with_context(retrieved_chunks: List[Dict]) -> List[Dict]:
    """L√†m gi√†u context v·ªõi parent, children, siblings"""
    chunk_map = vector_store["chunk_map"]
    enriched = []

    for item in retrieved_chunks:
        chunk = item["chunk"]
        similarity = item["similarity"]

        # Main chunk
        enriched.append({
            "type": "main",
            "chunk_id": chunk["chunk_id"],
            "section_code": chunk["metadata"]["section_code"],
            "section_title": chunk["metadata"]["section_title"],
            "content": chunk["content"],
            "similarity": similarity,
            "title_path": " > ".join(chunk["metadata"]["title_path"])
        })

        # Parent
        if config.INCLUDE_PARENT and chunk["metadata"]["parent_id"]:
            parent_id = chunk["metadata"]["parent_id"]
            if parent_id in chunk_map:
                parent = chunk_map[parent_id]
                enriched.append({
                    "type": "parent",
                    "chunk_id": parent["chunk_id"],
                    "section_code": parent["metadata"]["section_code"],
                    "section_title": parent["metadata"]["section_title"],
                    "content": parent["content"][:300] + "...",  # Ch·ªâ l·∫•y 300 k√Ω t·ª±
                    "title_path": " > ".join(parent["metadata"]["title_path"])
                })

        # Children/Descendants
        children_ids = chunk["metadata"]["children_ids"][:config.MAX_DESCENDANTS]
        for child_id in children_ids:
            if child_id in chunk_map:
                child = chunk_map[child_id]
                enriched.append({
                    "type": "child",
                    "chunk_id": child["chunk_id"],
                    "section_code": child["metadata"]["section_code"],
                    "section_title": child["metadata"]["section_title"],
                    "content": child["content"],
                    "title_path": " > ".join(child["metadata"]["title_path"])
                })

        # Siblings
        sibling_ids = chunk["metadata"]["sibling_ids"][:config.MAX_SIBLINGS]
        for sibling_id in sibling_ids:
            if sibling_id in chunk_map:
                sibling = chunk_map[sibling_id]
                enriched.append({
                    "type": "sibling",
                    "chunk_id": sibling["chunk_id"],
                    "section_code": sibling["metadata"]["section_code"],
                    "section_title": sibling["metadata"]["section_title"],
                    "content": sibling["content"][:200] + "...",  # Ch·ªâ l·∫•y 200 k√Ω t·ª±
                    "title_path": " > ".join(sibling["metadata"]["title_path"])
                })

    return enriched


def generate_answer(query: str, context_chunks: List[Dict]) -> str:
    """Sinh c√¢u tr·∫£ l·ªùi s·ª≠ d·ª•ng Gemini"""

    # T·∫°o context string
    context_parts = []
    for chunk in context_chunks:
        context_parts.append(
            f"[{chunk['type'].upper()}] {chunk['section_code']}: {chunk['section_title']}\n"
            f"Path: {chunk['title_path']}\n"
            f"Content: {chunk['content']}\n"
        )

    context_str = "\n---\n".join(context_parts)

    # Prompt
    prompt = f"""B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ t√†i li·ªáu h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng Tendoo App.

D·ª±a v√†o th√¥ng tin sau ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch chi ti·∫øt v√† ch√≠nh x√°c.

TH√îNG TIN T√ÄI LI·ªÜU:
{context_str}

C√ÇU H·ªéI: {query}

H∆Ø·ªöNG D·∫™N TR·∫¢ L·ªúI:
- Tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin trong t√†i li·ªáu
- N·∫øu c√≥ c√°c b∆∞·ªõc h∆∞·ªõng d·∫´n, h√£y tr√¨nh b√†y r√µ r√†ng theo th·ª© t·ª±
- N·∫øu c√≥ nhi·ªÅu m·ª•c li√™n quan, h√£y li·ªát k√™ ƒë·∫ßy ƒë·ªß
- N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin, h√£y n√≥i r√µ
- S·ª≠ d·ª•ng ti·∫øng Vi·ªát r√µ r√†ng, d·ªÖ hi·ªÉu

TR·∫¢ L·ªúI:"""

    try:
        model = genai.GenerativeModel(config.LLM_MODEL)
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        print(f"‚ùå L·ªói khi sinh c√¢u tr·∫£ l·ªùi: {e}")
        return f"Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra khi sinh c√¢u tr·∫£ l·ªùi: {str(e)}"


# ==================== API ENDPOINTS ====================

@app.on_event("startup")
async def startup_event():
    """Load chunks v√† t·∫°o embeddings khi kh·ªüi ƒë·ªông"""
    print("\n" + "="*80)
    print("KH·ªûI ƒê·ªòNG TENDOO RAG API")
    print("="*80)

    # Load chunks
    chunks = load_chunks()
    if not chunks:
        print("‚ö†Ô∏è Kh√¥ng c√≥ chunks n√†o ƒë∆∞·ª£c load!")
        return

    vector_store["chunks"] = chunks

    # T·∫°o chunk map
    chunk_map = {chunk["chunk_id"]: chunk for chunk in chunks}
    vector_store["chunk_map"] = chunk_map

    # T·∫°o embeddings
    embeddings = embed_chunks(chunks)
    vector_store["embeddings"] = embeddings

    print("\n‚úÖ S·∫µn s√†ng x·ª≠ l√Ω queries!")
    print("="*80 + "\n")


@app.get("/")
async def root():
    """Health check"""
    return {
        "status": "ok",
        "service": "Tendoo RAG API",
        "chunks_loaded": len(vector_store["chunks"]),
        "embeddings_created": len(vector_store["embeddings"])
    }


@app.post("/query", response_model=QueryResponse)
async def query(request: QueryRequest):
    """Query endpoint"""

    if not vector_store["chunks"]:
        raise HTTPException(status_code=500, detail="Chunks ch∆∞a ƒë∆∞·ª£c load")

    print(f"\nüìù Query: {request.query}")

    # T·∫°o query embedding
    query_embedding = create_embedding(request.query)
    if not query_embedding:
        raise HTTPException(status_code=500, detail="Kh√¥ng th·ªÉ t·∫°o query embedding")

    # Retrieve similar chunks
    retrieved = retrieve_similar_chunks(query_embedding, top_k=request.topK)
    print(f"üìä T√¨m th·∫•y {len(retrieved)} chunks t∆∞∆°ng ƒë·ªìng")

    # Enrich with context
    enriched = enrich_with_context(retrieved)
    print(f"üìö L√†m gi√†u th√†nh {len(enriched)} chunks (c√≥ parent/children/siblings)")

    # Generate answer
    answer = generate_answer(request.query, enriched)
    print(f"‚úÖ ƒê√£ sinh c√¢u tr·∫£ l·ªùi")

    # Prepare response
    retrieved_docs = []
    for item in retrieved:
        chunk = item["chunk"]
        retrieved_docs.append({
            "chunk_id": chunk["chunk_id"],
            "section_code": chunk["metadata"]["section_code"],
            "section_title": chunk["metadata"]["section_title"],
            "content": chunk["content"][:500] + "..." if len(chunk["content"]) > 500 else chunk["content"],
            "similarity": item["similarity"],
            "title_path": " > ".join(chunk["metadata"]["title_path"])
        })

    return QueryResponse(
        answer=answer,
        retrievedDocuments=retrieved_docs,
        metadata={
            "total_retrieved": len(retrieved),
            "total_enriched": len(enriched),
            "top_k": request.topK
        }
    )


@app.get("/stats")
async def stats():
    """Th·ªëng k√™ v·ªÅ h·ªá th·ªëng"""
    if not vector_store["chunks"]:
        return {"error": "Chunks ch∆∞a ƒë∆∞·ª£c load"}

    chunks = vector_store["chunks"]

    # Th·ªëng k√™ theo section type
    from collections import defaultdict
    type_counts = defaultdict(int)
    level_counts = defaultdict(int)
    tag_counts = defaultdict(int)

    for chunk in chunks:
        type_counts[chunk["metadata"]["section_type"]] += 1
        level_counts[chunk["metadata"]["level"]] += 1
        for tag in chunk["metadata"]["tags"]:
            tag_counts[tag] += 1

    return {
        "total_chunks": len(chunks),
        "by_section_type": dict(type_counts),
        "by_level": dict(level_counts),
        "top_tags": dict(sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:10])
    }


# ==================== MAIN ====================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
