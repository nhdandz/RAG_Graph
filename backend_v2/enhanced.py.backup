"""
Enhanced RAG API cho VÄƒn báº£n Tuyá»ƒn sinh - Version ÄÆ¡n Giáº£n, á»”n Äá»‹nh
Há»— trá»£:
- Load chunks tá»« JSON
- Hybrid search (Dense + BM25)
- Graph-based context enrichment (parent, children)
- Gemini 2.5 Flash vá»›i Ollama fallback
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional, Any
import numpy as np
import google.generativeai as genai
import ollama
from sklearn.metrics.pairwise import cosine_similarity
import json
import re
import math
from collections import Counter
from pathlib import Path

app = FastAPI(title="Admission RAG API - Enhanced")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Configuration
class Config:
    # Gemini API
    GEMINI_API_KEY = "AIzaSyBM2d63oq4whZNtQMxrCUd1KDtVItsSALA"
    LLM_MODEL = "gemini-2.5-flash"
    EMBEDDING_MODEL = "bge-m3"
    OLLAMA_FALLBACK_MODEL = "qwen3:8b"

    # Feature flags
    USE_HYBRID_SEARCH = True
    USE_GRAPH_ENRICHMENT = True
    USE_DEDUPLICATION = True

    # Parameters
    DEDUP_THRESHOLD = 0.90
    BM25_K1 = 1.5
    BM25_B = 0.75
    RRF_K = 60

    # Context enrichment settings
    MAX_PARENT_LEVELS = 2
    MAX_CHILDREN = 10
    MAX_SIBLINGS = 2

config = Config()

# Configure Gemini API
genai.configure(api_key=config.GEMINI_API_KEY)


# In-memory storage
vector_store = {
    "chunks": [],
    "embeddings": [],
    "chunk_map": {}
}


# Pydantic Models
class QueryRequest(BaseModel):
    query: str
    topK: int = 3


class ContextChunk(BaseModel):
    type: str
    heading: str
    headingPath: str
    content: str
    similarity: Optional[float] = None
    importance: Optional[float] = None
    relatedTo: Optional[str] = None
    relationshipType: Optional[str] = None


class QueryResponse(BaseModel):
    answer: str
    retrievedDocuments: List[Dict[str, Any]]
    contextStructure: Dict[str, Any]


# BM25 Implementation
class BM25:
    """BM25 sparse retrieval"""

    @staticmethod
    def tokenize(text: str) -> List[str]:
        if not text:
            return []
        cleaned = re.sub(r'[^\w\s]', ' ', text.lower())
        tokens = [word for word in cleaned.split() if len(word) > 1]
        return tokens

    @staticmethod
    def calculate_idf(query_terms: List[str], documents: List[str]) -> Dict[str, float]:
        idf_scores = {}
        total_docs = len(documents)

        for term in query_terms:
            docs_with_term = sum(1 for doc in documents if term in BM25.tokenize(doc))
            idf = math.log((total_docs - docs_with_term + 0.5) / (docs_with_term + 0.5) + 1.0)
            idf_scores[term] = idf

        return idf_scores

    @staticmethod
    def calculate_bm25_scores(query: str, documents: List[str], k1: float = 1.5, b: float = 0.75) -> List[float]:
        query_terms = BM25.tokenize(query)
        if not query_terms:
            return [0.0] * len(documents)

        doc_lengths = [len(BM25.tokenize(doc)) for doc in documents]
        avg_doc_length = sum(doc_lengths) / len(documents) if documents else 0

        idf_scores = BM25.calculate_idf(query_terms, documents)

        scores = []
        for doc, doc_len in zip(documents, doc_lengths):
            doc_terms = BM25.tokenize(doc)
            term_freq = Counter(doc_terms)

            score = 0.0
            for term in query_terms:
                if term in term_freq:
                    tf = term_freq[term]
                    idf = idf_scores[term]
                    numerator = tf * (k1 + 1)
                    denominator = tf + k1 * (1 - b + b * doc_len / avg_doc_length)
                    score += idf * (numerator / denominator)

            scores.append(score)

        return scores


def reciprocal_rank_fusion(dense_scores: List[float], sparse_scores: List[float], k: int = 60) -> List[float]:
    """Reciprocal Rank Fusion"""
    dense_ranks = np.argsort(dense_scores)[::-1]
    sparse_ranks = np.argsort(sparse_scores)[::-1]

    dense_rank_map = {idx: rank for rank, idx in enumerate(dense_ranks)}
    sparse_rank_map = {idx: rank for rank, idx in enumerate(sparse_ranks)}

    rrf_scores = []
    for i in range(len(dense_scores)):
        dense_rank = dense_rank_map.get(i, len(dense_scores))
        sparse_rank = sparse_rank_map.get(i, len(sparse_scores))
        rrf_score = (1.0 / (k + dense_rank)) + (1.0 / (k + sparse_rank))
        rrf_scores.append(rrf_score)

    return rrf_scores


def deduplicate_chunks(chunks: List[Dict], threshold: float = 0.90) -> List[Dict]:
    """Remove duplicate chunks using Jaccard similarity"""
    unique_chunks = []

    for chunk in chunks:
        is_duplicate = False
        chunk_words = set(chunk['content'].lower().split())

        for unique_chunk in unique_chunks:
            unique_words = set(unique_chunk['content'].lower().split())

            intersection = chunk_words & unique_words
            union = chunk_words | unique_words

            if union:
                jaccard = len(intersection) / len(union)
                if jaccard > threshold:
                    is_duplicate = True
                    break

        if not is_duplicate:
            unique_chunks.append(chunk)

    return unique_chunks


def get_embedding(text: str) -> np.ndarray:
    """Get embedding from Ollama BGE-M3"""
    try:
        response = ollama.embed(model=config.EMBEDDING_MODEL, input=text)
        return np.array(response['embeddings'][0])
    except Exception as e:
        print(f"Embedding error: {e}")
        raise HTTPException(status_code=500, detail=f"Embedding error: {str(e)}")


def find_parent_chunks(chunk: Dict, max_levels: int = 2) -> List[Dict]:
    """TÃ¬m cÃ¡c chunk cha"""
    parents = []
    current_chunk = chunk

    for _ in range(max_levels):
        parent_id = current_chunk.get('metadata', {}).get('parent_id')
        if not parent_id or parent_id not in vector_store['chunk_map']:
            break

        parent_chunk = vector_store['chunk_map'][parent_id]
        parents.append(parent_chunk)
        current_chunk = parent_chunk

    return parents


def find_all_descendants(chunk: Dict) -> List[Dict]:
    """TÃ¬m táº¥t cáº£ con chÃ¡u cá»§a chunk"""
    all_descendants = []
    visited = set()

    def collect_descendants(current_chunk: Dict):
        children_ids = current_chunk.get('metadata', {}).get('children_ids', [])

        for child_id in children_ids:
            if child_id in visited or child_id not in vector_store['chunk_map']:
                continue

            visited.add(child_id)
            child_chunk = vector_store['chunk_map'][child_id]
            all_descendants.append(child_chunk)

            # Äá»‡ quy
            collect_descendants(child_chunk)

    collect_descendants(chunk)
    return all_descendants


def find_sibling_chunks(chunk: Dict, max_siblings: int = 2) -> List[Dict]:
    """TÃ¬m cÃ¡c chunk anh em"""
    sibling_ids = chunk.get('metadata', {}).get('sibling_ids', [])
    siblings = []

    for sibling_id in sibling_ids[:max_siblings]:
        if sibling_id in vector_store['chunk_map']:
            siblings.append(vector_store['chunk_map'][sibling_id])

    return siblings


def build_enriched_chunk_content(chunk: Dict, max_children: int = 10) -> str:
    """
    LÃ m giÃ u ná»™i dung cá»§a 1 chunk báº±ng cÃ¡ch ghÃ©p parent + children
    """
    enriched_parts = []
    metadata = chunk['metadata']

    # 1. ThÃªm ngá»¯ cáº£nh CHA (náº¿u cÃ³)
    if config.USE_GRAPH_ENRICHMENT:
        parents = find_parent_chunks(chunk, config.MAX_PARENT_LEVELS)
        if parents:
            enriched_parts.append("ã€ NGá»® Cáº¢NH Tá»”NG QUÃT ã€‘")
            for i, parent in enumerate(parents, 1):
                parent_title = parent['metadata'].get('section_title', 'KhÃ´ng rÃµ')
                parent_path = ' > '.join(parent['metadata'].get('title_path', []))
                enriched_parts.append(f"\nğŸ“‚ Cáº¥p cha {i}: {parent_title}")
                enriched_parts.append(f"   Vá»‹ trÃ­: {parent_path}")
                enriched_parts.append(f"   Ná»™i dung: {parent['content'][:300]}...")
            enriched_parts.append("")

    # 2. Ná»˜I DUNG CHÃNH
    section_title = metadata.get('section_title', 'KhÃ´ng rÃµ')
    title_path = ' > '.join(metadata.get('title_path', []))

    enriched_parts.append("ã€ Ná»˜I DUNG CHÃNH ã€‘")
    enriched_parts.append(f"ğŸ“Œ TiÃªu Ä‘á»: {section_title}")
    enriched_parts.append(f"ğŸ“ Vá»‹ trÃ­: {title_path}")
    enriched_parts.append(f"\n{chunk['content']}")
    enriched_parts.append("")

    # 3. ThÃªm CÃC Má»¤C CON (giá»›i háº¡n sá»‘ lÆ°á»£ng)
    if config.USE_GRAPH_ENRICHMENT:
        all_descendants = find_all_descendants(chunk)
        if all_descendants:
            # Giá»›i háº¡n sá»‘ lÆ°á»£ng children
            limited_descendants = all_descendants[:max_children]
            enriched_parts.append(f"ã€ CÃC Má»¤C CON ({len(limited_descendants)}/{len(all_descendants)}) ã€‘")
            for i, descendant in enumerate(limited_descendants, 1):
                desc_meta = descendant['metadata']
                desc_title = desc_meta.get('section_title', 'KhÃ´ng rÃµ')
                desc_path = ' > '.join(desc_meta.get('title_path', []))
                desc_level = desc_meta.get('level', 0)

                indent = "  " * (desc_level - metadata.get('level', 0))

                enriched_parts.append(f"\n{indent}ğŸ”¸ [{i}] {desc_title}")
                enriched_parts.append(f"{indent}   ğŸ“ {desc_path}")
                enriched_parts.append(f"{indent}   {descendant['content']}")
            enriched_parts.append("")

    return "\n".join(enriched_parts)


def build_context_structure(primary_chunks: List[Dict], max_children: int = 10) -> List[ContextChunk]:
    """
    Chá»‰ tráº£ vá» 1 CHUNK DUY NHáº¤T Ä‘Ã£ Ä‘Æ°á»£c lÃ m giÃ u
    """
    if not primary_chunks:
        return []

    # Chá»‰ láº¥y chunk Ä‘áº§u tiÃªn (cÃ³ similarity cao nháº¥t)
    chunk = primary_chunks[0]
    metadata = chunk['metadata']
    title_path = ' > '.join(metadata.get('title_path', []))

    # LÃ m giÃ u content
    enriched_content = build_enriched_chunk_content(chunk, max_children)

    return [ContextChunk(
        type="primary",
        heading=metadata.get('section_title', ''),
        headingPath=title_path,
        content=enriched_content,
        similarity=chunk.get('similarity', 0.0),
        importance=1.0
    )]


def generate_answer(query: str, context_chunks: List[ContextChunk]) -> str:
    """Táº¡o cÃ¢u tráº£ lá»i tá»« LLM"""
    if not context_chunks:
        return "KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan."

    chunk = context_chunks[0]

    context = f"""â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TÃ€I LIá»†U THAM KHáº¢O
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ ÄÆ°á»ng dáº«n: {chunk.headingPath}
â­ Äá»™ liÃªn quan: {chunk.similarity:.2%}

{chunk.content}
"""

    prompt = f"""Báº¡n lÃ  trá»£ lÃ½ áº£o há»— trá»£ giáº£i Ä‘Ã¡p vá» ThÃ´ng tÆ° tuyá»ƒn sinh cá»§a Há»c viá»‡n QuÃ¢n sá»±.

NHIá»†M Vá»¤:
1. Tráº£ lá»i cÃ¢u há»i dá»±a trÃªn tÃ i liá»‡u tham kháº£o bÃªn dÆ°á»›i
2. TrÃ­ch dáº«n rÃµ rÃ ng Ä‘iá»u khoáº£n, khoáº£n, má»¥c liÃªn quan
3. Giáº£i thÃ­ch dá»… hiá»ƒu, chuyÃªn nghiá»‡p báº±ng tiáº¿ng Viá»‡t

HÆ¯á»šNG DáºªN:
- TÃ i liá»‡u Ä‘Ã£ bao gá»“m:
  â€¢ Ngá»¯ cáº£nh tá»•ng quÃ¡t (pháº§n cha)
  â€¢ Ná»™i dung chÃ­nh (pháº§n liÃªn quan nháº¥t)
  â€¢ CÃ¡c má»¥c con (chi tiáº¿t Ä‘áº§y Ä‘á»§)

- Sá»­ dá»¥ng thÃ´ng tin tá»« táº¥t cáº£ cÃ¡c pháº§n
- Náº¿u khÃ´ng Ä‘á»§ thÃ´ng tin, nÃ³i rÃµ "ThÃ´ng tin nÃ y khÃ´ng cÃ³ trong tÃ i liá»‡u hiá»‡n cÃ³"
- TrÃ­ch dáº«n: "Theo Äiá»u X, Khoáº£n Y..."
- Tráº£ lá»i chi tiáº¿t, Ä‘áº§y Ä‘á»§

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TÃ€I LIá»†U THAM KHáº¢O
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{context}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
CÃ‚U Há»I
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{query}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TRáº¢ Lá»œI
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
    print(prompt)
    try:
        # Try Gemini first
        print("ğŸ¤– Using Gemini 2.5 Flash...")
        model = genai.GenerativeModel(config.LLM_MODEL)
        response = model.generate_content(
            prompt,
            generation_config=genai.GenerationConfig(
                temperature=0.3,
                top_p=0.9,
                max_output_tokens=2048,
            ),
            request_options={"timeout": 60}
        )
        return response.text
    except Exception as e:
        print(f"âš ï¸ Gemini error: {e}")
        print(f"ğŸ”„ Falling back to Ollama {config.OLLAMA_FALLBACK_MODEL}...")

        try:
            # Fallback to Ollama
            response = ollama.chat(
                model=config.OLLAMA_FALLBACK_MODEL,
                messages=[{
                    'role': 'user',
                    'content': prompt
                }],
                options={
                    'temperature': 0.3,
                    'top_p': 0.9,
                    'num_predict': 2048
                }
            )
            return response['message']['content']
        except Exception as ollama_error:
            print(f"âŒ Ollama error: {ollama_error}")
            raise HTTPException(
                status_code=500,
                detail=f"Both LLM providers failed. Gemini: {str(e)}, Ollama: {str(ollama_error)}"
            )


@app.post("/api/documents/load-from-json")
async def load_from_json(json_file_path: str = "output_admission/chunks.json"):
    """Load chunks tá»« JSON"""
    try:
        print(f"\n{'='*80}")
        print(f"ğŸ“‚ Loading chunks from: {json_file_path}")
        print(f"{'='*80}")

        json_path = Path(__file__).parent.parent / json_file_path
        if not json_path.exists():
            json_path = Path(json_file_path)

        if not json_path.exists():
            raise HTTPException(status_code=404, detail=f"JSON file not found: {json_file_path}")

        with open(json_path, 'r', encoding='utf-8') as f:
            chunks_data = json.load(f)

        if not isinstance(chunks_data, list):
            raise HTTPException(status_code=400, detail="JSON must be a list of chunks")

        print(f"\nâœ“ Found {len(chunks_data)} chunks in JSON")

        vector_store['chunks'].clear()
        vector_store['embeddings'].clear()
        vector_store['chunk_map'].clear()

        chunks_added = 0
        chunk_sizes = []

        for chunk_data in chunks_data:
            content = chunk_data.get('content', '').strip()
            if not content:
                continue

            metadata = chunk_data.get('metadata', {})
            word_count = metadata.get('word_count', len(content.split()))
            chunk_sizes.append(word_count)

            embedding = get_embedding(content)

            chunk_obj = {
                'chunk_id': chunk_data['chunk_id'],
                'content': content,
                'metadata': metadata,
                'similarity': 0.0
            }

            vector_store['chunks'].append(chunk_obj)
            vector_store['embeddings'].append(embedding)
            vector_store['chunk_map'][chunk_data['chunk_id']] = chunk_obj

            chunks_added += 1

            if chunks_added <= 10 or chunks_added % 20 == 0:
                section_code = metadata.get('section_code', '')
                section_title = metadata.get('section_title', '')
                print(f"  âœ“ [{section_code:15s}] {section_title[:50]:50s} ({word_count:4d} words)")

        print(f"\n{'='*80}")
        print(f"âœ… Loaded {chunks_added} chunks successfully")
        print(f"{'='*80}\n")

        return {
            "message": f"Loaded {chunks_added} chunks",
            "totalDocuments": len(vector_store['chunks']),
            "chunksAdded": chunks_added,
            "chunkStats": {
                "min": min(chunk_sizes) if chunk_sizes else 0,
                "max": max(chunk_sizes) if chunk_sizes else 0,
                "avg": sum(chunk_sizes) / len(chunk_sizes) if chunk_sizes else 0,
                "total": chunks_added
            }
        }

    except Exception as e:
        print(f"\nâŒ Error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/query", response_model=QueryResponse)
async def query_documents(request: QueryRequest):
    """Query vá»›i Hybrid Search + Graph Enrichment"""
    try:
        if not vector_store['chunks']:
            raise HTTPException(status_code=400, detail="No documents indexed yet")

        print(f"\n{'='*80}")
        print(f"Query: {request.query}")
        print(f"{'='*80}\n")

        # Get query embedding
        query_embedding = get_embedding(request.query)

        # Stage 1: Hybrid Search
        embeddings = np.array(vector_store['embeddings'])
        dense_scores = cosine_similarity(query_embedding.reshape(1, -1), embeddings)[0]

        if config.USE_HYBRID_SEARCH:
            documents_text = [chunk['content'] for chunk in vector_store['chunks']]
            sparse_scores = BM25.calculate_bm25_scores(request.query, documents_text,
                                                       config.BM25_K1, config.BM25_B)

            fused_scores = reciprocal_rank_fusion(dense_scores.tolist(), sparse_scores, config.RRF_K)
            top_indices = np.argsort(fused_scores)[-request.topK * 2:][::-1]
            print(f"Stage 1 - Hybrid search: {len(top_indices)} candidates")
        else:
            top_indices = np.argsort(dense_scores)[-request.topK * 2:][::-1]
            print(f"Stage 1 - Dense search: {len(top_indices)} candidates")

        # Get candidate chunks
        candidate_chunks = []
        for idx in top_indices:
            chunk = vector_store['chunks'][idx].copy()
            chunk['similarity'] = float(dense_scores[idx])
            candidate_chunks.append(chunk)

        # Stage 2: Deduplication
        if config.USE_DEDUPLICATION:
            candidate_chunks = deduplicate_chunks(candidate_chunks, config.DEDUP_THRESHOLD)
            print(f"Stage 2 - After dedup: {len(candidate_chunks)} chunks")

        # Get top K candidates
        final_chunks = candidate_chunks[:request.topK]
        print(f"Stage 3 - Top candidates: {len(final_chunks)} chunks")

        # Stage 4: Build 1 SINGLE enriched chunk
        context_structure = build_context_structure(final_chunks, config.MAX_CHILDREN)

        if context_structure:
            all_descendants = find_all_descendants(final_chunks[0]) if final_chunks else []
            print(f"\nğŸ“Š Context Structure:")
            print(f"  âœ“ Chá»n 1 CHUNK CHÃNH cÃ³ Ä‘á»™ liÃªn quan cao nháº¥t")
            print(f"  âœ“ ÄÃ£ ghÃ©p thÃªm {min(len(all_descendants), config.MAX_CHILDREN)}/{len(all_descendants)} má»¥c con")
            print(f"  âœ“ Tá»•ng sá»‘ chunks gá»­i cho LLM: {len(context_structure)} (1 chunk Ä‘Ã£ lÃ m giÃ u)\n")

        # Generate answer
        answer = generate_answer(request.query, context_structure)

        # Prepare retrieved docs
        retrieved_docs = []
        for chunk in final_chunks:
            metadata = chunk['metadata']
            retrieved_docs.append({
                "filename": "ThÃ´ng tÆ° tuyá»ƒn sinh",
                "content": chunk['content'],
                "similarity": chunk['similarity'],
                "section_code": metadata.get('section_code', ''),
                "section_title": metadata.get('section_title', '')
            })

        return QueryResponse(
            answer=answer,
            retrievedDocuments=retrieved_docs,
            contextStructure={
                "chunks": [chunk.dict() for chunk in context_structure],
                "summary": {
                    "primaryCount": sum(1 for c in context_structure if c.type == 'primary'),
                    "parentCount": sum(1 for c in context_structure if c.type == 'parent_context'),
                    "relatedCount": sum(1 for c in context_structure if c.type == 'semantic_neighbor'),
                    "totalCount": len(context_structure)
                }
            }
        )

    except Exception as e:
        print(f"\nâŒ Query error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/health")
async def health_check():
    """Health check"""
    return {
        "status": "ok",
        "service": "Admission RAG API - Enhanced (Simple & Stable)",
        "embedding_model": config.EMBEDDING_MODEL + " (Ollama)",
        "llm_model": config.LLM_MODEL,
        "llm_fallback": config.OLLAMA_FALLBACK_MODEL,
        "features": {
            "hybrid_search": config.USE_HYBRID_SEARCH,
            "graph_enrichment": config.USE_GRAPH_ENRICHMENT,
            "deduplication": config.USE_DEDUPLICATION
        },
        "documents": len(vector_store['chunks'])
    }


@app.get("/api/documents/count")
async def get_document_count():
    """Get total chunks"""
    return {"count": len(vector_store['chunks'])}


@app.delete("/api/documents")
async def clear_documents():
    """Clear all documents"""
    vector_store['chunks'].clear()
    vector_store['embeddings'].clear()
    vector_store['chunk_map'].clear()
    return {"message": "All documents cleared successfully"}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080)
